# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jiwvbWYThiGZbmt0jSYR6lOLVLO-gyuk
"""

# Cleaned version with all comments removed
import os
import math
import random
from typing import Tuple, Dict, Any, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import statsmodels.api as sm

try:
    import shap
    SHAP_AVAILABLE = True
except Exception:
    SHAP_AVAILABLE = False

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

OUTPUT_DIR = 'outputs'
os.makedirs(OUTPUT_DIR, exist_ok=True)


def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)


def rmse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))


def mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0


def generate_complex_multivariate_series(n_steps=2000, n_vars=3) -> pd.DataFrame:
    t = np.arange(n_steps)
    seasonal1 = 10 * np.sin(2 * np.pi * t / 24)
    seasonal2 = 5 * np.sin(2 * np.pi * t / 168)
    trend = 0.01 * t

    data = []
    for i in range(n_vars):
        noise = np.random.normal(scale=1.0 + 0.2 * i, size=n_steps)
        amp = 1.0 + 0.5 * i
        regime = np.zeros(n_steps)
        regime[int(0.4 * n_steps):] += (i + 1) * 2.0
        regime[int(0.8 * n_steps):] -= (i + 1) * 1.5
        coupling = np.zeros(n_steps)
        if i > 0:
            coupling = 0.3 * np.sin(2 * np.pi * (t - 2 * i) / (24 + 6 * i))
        series = amp * seasonal1 + (amp * 0.5) * seasonal2 + trend + noise + regime + coupling + i * 0.2 * t / len(t)
        data.append(series)

    df = pd.DataFrame(np.vstack(data).T, columns=[f'var_{i}' for i in range(n_vars)])
    date_index = pd.date_range(start='2020-01-01', periods=n_steps, freq='H')
    df.index = date_index
    return df


class TimeSeriesDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def create_windows(data: np.ndarray, input_len: int, horizon: int, stride: int = 1):
    T, n_vars = data.shape
    Xs, ys = [], []
    for start in range(0, T - input_len - horizon + 1, stride):
        end = start + input_len
        Xs.append(data[start:end])
        ys.append(data[end:end + horizon])
    return np.array(Xs), np.array(ys)


class StackedLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout, output_dim, horizon):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, max(hidden_dim // 2, 1)),
            nn.ReLU(),
            nn.Linear(max(hidden_dim // 2, 1), output_dim * horizon)
        )
        self.output_dim = output_dim
        self.horizon = horizon

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        out = self.fc(last)
        return out.view(-1, self.horizon, self.output_dim)


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, : x.size(1), :]


class TransformerForecast(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout, horizon):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        self.pos_enc = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output = nn.Linear(d_model, input_dim * horizon)
        self.d_model = d_model
        self.horizon = horizon
        self.input_dim = input_dim

    def forward(self, x):
        x = self.input_proj(x) * math.sqrt(self.d_model)
        x = self.pos_enc(x)
        x = x.permute(1, 0, 2)
        encoded = self.transformer_encoder(x)
        last = encoded[-1]
        out = self.output(last)
        return out.view(-1, self.horizon, self.input_dim)


class TransformerWithAttention(TransformerForecast):
    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout, horizon):
        super().__init__(input_dim, d_model, nhead, num_layers, dim_feedforward, dropout, horizon)
        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)

    def forward(self, x, return_attn: bool = False):
        x_proj = self.input_proj(x) * math.sqrt(self.d_model)
        x_pos = self.pos_enc(x_proj)
        attn_output, attn_weights = self.mha(x_pos, x_pos, x_pos, need_weights=True)
        x_t = x_pos.permute(1, 0, 2)
        encoded_full = self.transformer_encoder(x_t)
        last = encoded_full[-1]
        out = self.output(last)
        out = out.view(-1, self.horizon, self.input_dim)
        if return_attn:
            return out, attn_weights.detach().cpu().numpy()
        return out


def train_model(model, train_loader, val_loader, epochs, lr, weight_decay=0.0, patience=10):
    model = model.to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.MSELoss()

    best_val_loss = float('inf')
    best_state = None
    epochs_no_improve = 0
    history = {'train_loss': [], 'val_loss': []}

    for epoch in range(1, epochs + 1):
        model.train()
        train_losses = []
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            preds = model(Xb)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        val_losses = []
        model.eval()
        with torch.no_grad():
            for Xv, yv in val_loader:
                Xv, yv = Xv.to(DEVICE), yv.to(DEVICE)
                pv = model(Xv)
                val_losses.append(criterion(pv, yv).item())

        train_loss = float(np.mean(train_losses)) if train_losses else float('nan')
        val_loss = float(np.mean(val_losses)) if val_losses else float('nan')
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = {k: v.cpu() for k, v in model.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                break

    if best_state is not None:
        model.load_state